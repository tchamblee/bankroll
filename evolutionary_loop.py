import pandas as pd
import numpy as np
import os
import shutil
import json
import random
import time
import config
import scipy.stats
import warnings

# Suppress annoying joblib/loky cleanup warnings
warnings.filterwarnings("ignore", category=UserWarning, module="joblib.externals.loky.backend.resource_tracker")

from feature_engine import FeatureEngine
from genome import GenomeFactory, Strategy
from backtest import BacktestEngine
from backtest.statistics import deflated_sharpe_ratio, estimated_sharpe_ratio
import uuid

class EvolutionaryAlphaFactory:
    def __init__(self, data, survivors_file, population_size=None, generations=100, decay_rate=0.99, target_col='log_ret', prediction_mode=False):
        self.training_id = uuid.uuid4().hex[:8]
        print(f"üÜî Training Run ID: {self.training_id}")
        
        self.data = data
        self.pop_size = population_size if population_size else config.EVO_BATCH_SIZE
        self.generations = generations
        self.decay_rate = decay_rate
        self.survivors_file = survivors_file
        self.prediction_mode = prediction_mode
        
        self.backtester = BacktestEngine(
            data, 
            cost_bps=config.COST_BPS, 
            target_col=target_col,
            annualization_factor=config.ANNUALIZATION_FACTOR,
            account_size=config.ACCOUNT_SIZE
        )
        
        self.factory = GenomeFactory(survivors_file)
        train_data = data.iloc[:self.backtester.train_idx]
        self.factory.set_stats(train_data)
        
        self.population = []
        # HOF now stores dicts: {'strat': Strategy, 'fit': float, 'sig': np.array, 'gen': int}
        # This list ensures DIVERSITY. We do not allow correlated strategies (>0.7) to coexist here.
        self.hall_of_fame = [] 
        
        # Track total unique strategies evaluated for DSR calculation
        self.total_strategies_evaluated = 0

    def cleanup(self):
        if hasattr(self, 'backtester'):
            self.backtester.shutdown()

    def initialize_population(self, horizon=None):
        print(f"  üé≤ Initializing Population with {self.pop_size} Random Strategies.")
        # SMALL DATA MODE: Max genes to prevent overfitting (controlled by config)
        self.population = [self.factory.create_strategy((config.GENE_COUNT_MIN, config.GENE_COUNT_MAX)) for _ in range(self.pop_size)]

    def crossover(self, p1, p2):
        child = Strategy(name=f"Child_{random.randint(1000,9999)}")
        
        n_long = random.randint(config.GENE_COUNT_MIN, config.GENE_COUNT_MAX)
        n_short = random.randint(config.GENE_COUNT_MIN, config.GENE_COUNT_MAX)
        
        combined_long = p1.long_genes + p2.long_genes
        combined_short = p1.short_genes + p2.short_genes
        
        child.long_genes = [g.copy() for g in random.sample(combined_long, min(len(combined_long), n_long))]
        child.short_genes = [g.copy() for g in random.sample(combined_short, min(len(combined_short), n_short))]
        
        child.recalculate_concordance()
        return child

    def update_hall_of_fame(self, candidates, gen, horizon=None):
        """
        Intelligently updates the Hall of Fame with new candidates.
        Enforces diversity: A new candidate is only added if it is UNCORRELATED with existing HOF entries,
        OR if it is correlated but strictly BETTER (higher fitness).
        """
        if not candidates: return
        
        # 1. Generate Signals for Candidates (Validation Slice)
        # We use the full available development set (Train+Val) for correlation to be robust
        # Using just Val might be too short/noisy.
        limit_idx = self.backtester.val_idx
        
        # We need to temporarily ensure context for these candidates
        # self.backtester.ensure_context(candidates) # generated by generate_signal_matrix
        
        cand_signals_full = self.backtester.generate_signal_matrix(candidates)
        cand_signals = cand_signals_full[:limit_idx]
        
        # Expectancy Filter (Churn Prevention)
        rets_batch, trades_batch = self.backtester.run_simulation_batch(
            cand_signals, 
            candidates, 
            self.backtester.open_vec[:limit_idx], 
            self.backtester.times_vec[:limit_idx],
            time_limit=horizon,
            highs=self.backtester.high_vec[:limit_idx],
            lows=self.backtester.low_vec[:limit_idx],
            atr=self.backtester.atr_vec[:limit_idx]
        )

        for i, cand in enumerate(candidates):
            if cand.fitness < 0.1: continue # Ignore junk
            
            n_trades = trades_batch[i]
            total_ret = np.sum(rets_batch[:, i])

            # Adaptive Expectancy Filter (Ramp up requirements)
            # Phase 1 (0-10%): Just positive (Survival)
            # Phase 2 (10-30%): 1 bps (Growth)
            # Phase 3 (30%+): 2 bps (Maturity)
            phase_1 = max(5, int(self.generations * 0.30)) # Extended Phase 1
            phase_2 = max(10, int(self.generations * 0.60))
            
            # Relaxed HOF Entry for early generations
            min_fitness = -10.0 if gen < phase_1 else 0.1
            
            if cand.fitness < min_fitness: 
                # print(f"    Rejected {cand.name}: Fitness {cand.fitness:.4f} < {min_fitness}")
                continue

            if gen < phase_1:
                thresh = -0.0005 # Allow loss (-5bps) in early game
            elif gen < phase_2:
                thresh = 0.0 # Breakeven
            else:
                thresh = 0.00005  # 0.5 bps

            if n_trades > 0:
                avg_ret = total_ret / n_trades
                if avg_ret < thresh:
                    # print(f"    Rejected {cand.name}: Avg Ret {avg_ret:.6f} < {thresh}")
                    continue
            else:
                continue # No trades = No alpha
            
            cand_sig = cand_signals[:, i]
            cand_fit = cand.fitness
            
            # Check against existing HOF
            is_unique = True
            
            # We iterate through the HOF. 
            # If we find a correlated strategy, we compare fitness.
            for hof_entry in self.hall_of_fame:
                existing_sig = hof_entry['sig']
                
                # Fast Correlation Check
                if np.std(cand_sig) == 0 or np.std(existing_sig) == 0:
                    corr = 0.0
                else:
                    corr = np.corrcoef(cand_sig, existing_sig)[0, 1]
                
                if corr > 0.70: # Correlation Threshold (0.7 = Strong Similarity)
                    is_unique = False
                    
                    # Found a cluster match. Is the new one better?
                    if cand_fit > hof_entry['fit']:
                        old_fit = hof_entry['fit']
                        # Replace the existing champion of this cluster
                        hof_entry['strat'] = cand
                        hof_entry['fit'] = cand_fit
                        hof_entry['sig'] = cand_sig
                        hof_entry['gen'] = gen
                        print(f"    ‚ôªÔ∏è  HOF Cluster Replacement: {cand.name} (Corr: {corr:.2f} | Fit: {old_fit:.2f} -> {cand_fit:.2f})")
                    break # Stop checking, we handled this cluster
            
            if is_unique:
                # Add new distinct alpha source
                self.hall_of_fame.append({
                    'strat': cand,
                    'fit': cand_fit,
                    'sig': cand_sig,
                    'gen': gen
                })
                print(f"    ‚≠ê HOF New Alpha: {cand.name} (Fit: {cand_fit:.2f})")

        # Sort HOF by Fitness
        self.hall_of_fame.sort(key=lambda x: x['fit'], reverse=True)
        
        # Cap Size (Keep top 500 DISTINCT strategies)
        if len(self.hall_of_fame) > 500:
            self.hall_of_fame = self.hall_of_fame[:500]

    def evolve(self, horizon=60):
        # random.seed(42)  <-- Removed to ensure diversity across runs
        # np.random.seed(42) <-- Removed to ensure diversity across runs
        
        self.initialize_population(horizon=horizon)
        
        generations_without_improvement = 0
        global_best_fitness = -999.0
        
        for gen in range(self.generations):
            start_time = time.time()
            
            # Dynamic Trade Filter (Horizon-Aware Scaling)
            target_final = max(10, int(config.MIN_TRADES_COEFFICIENT / horizon + 5))
            scaling_factor = target_final / 10.0
            
            current_min_trades = int(3 * scaling_factor)
            if gen >= 5: current_min_trades = int(6 * scaling_factor)
            if gen >= 15: current_min_trades = target_final
            
            # Increment trial counter for DSR
            self.total_strategies_evaluated += len(self.population)

            # 1. Evaluate using Rolling Walk-Forward Validation
            wfv_results = self.backtester.evaluate_walk_forward(self.population, folds=4, time_limit=horizon, min_trades=current_min_trades)
            wfv_scores = wfv_results['sortino'].values
            
            # DEBUG: Print Stats
            avg_wfv = np.mean(wfv_scores)
            avg_trades_debug = wfv_results.get('avg_trades', pd.Series([0]*len(self.population))).mean()
            # Calculate roughly average return from wfv_results if possible, or just skip it.
            # wfv_results doesn't return raw returns.
            # Let's just print what we have.
            print(f"  DEBUG Gen {gen}: Avg Raw WFV: {avg_wfv:.2f} | Avg Trades: {avg_trades_debug:.1f}")

            # 2. Penalties (Dominance & Complexity)
            feature_counts = {}
            for strat in self.population:
                used_features = set()
                for gene in strat.long_genes + strat.short_genes:
                    if hasattr(gene, 'feature'): used_features.add(gene.feature)
                    elif hasattr(gene, 'feature_left'): 
                        used_features.add(gene.feature_left)
                        used_features.add(gene.feature_right)
                for f in used_features:
                    feature_counts[f] = feature_counts.get(f, 0) + 1
            
            for i, strat in enumerate(self.population):
                n_genes = len(strat.long_genes) + len(strat.short_genes)
                complexity_penalty = n_genes * config.COMPLEXITY_PENALTY_PER_GENE
                
                dom_penalty = 0.0
                strat_features = set()
                for gene in strat.long_genes + strat.short_genes:
                    if hasattr(gene, 'feature'): strat_features.add(gene.feature)
                    elif hasattr(gene, 'feature_left'): 
                        strat_features.add(gene.feature_left)
                        strat_features.add(gene.feature_right)
                
                for f in strat_features:
                    count = feature_counts.get(f, 0)
                    ratio = count / self.pop_size
                    if ratio > config.DOMINANCE_PENALTY_THRESHOLD:
                         dom_penalty += (ratio - config.DOMINANCE_PENALTY_THRESHOLD) * config.DOMINANCE_PENALTY_MULTIPLIER
                
                # Quadratic Complexity Penalty: Punish bloat progressively harder
                # 4 genes -> 16 * C (Small)
                # 8 genes -> 64 * C (Huge)
                # We scale by 0.5 to keep the base impact reasonable for small strategies
                complexity_penalty = 0.5 * (n_genes ** 2) * config.COMPLEXITY_PENALTY_PER_GENE
                
                wfv_scores[i] -= (complexity_penalty + dom_penalty)
                strat.fitness = wfv_scores[i]

            # 3. Stats & Early Stopping
            best_idx = np.argmax(wfv_scores)
            best_fitness = wfv_scores[best_idx]
            
            if gen % 5 == 0:
                print(f"Gen {gen} | Best WFV Score: {best_fitness:.4f}")
            
            if best_fitness > global_best_fitness:
                global_best_fitness = best_fitness
                generations_without_improvement = 0
            else:
                generations_without_improvement += 1
                
            if generations_without_improvement >= 15:
                print(f"üõë Early Stopping Triggered.")
                break
            
            # 4. Update Diverse Hall of Fame
            sorted_indices = np.argsort(wfv_scores)[::-1]
            top_50_indices = sorted_indices[:50]
            top_50_strats = [self.population[i] for i in top_50_indices]
            
            # Tag generation
            for s in top_50_strats:
                if not hasattr(s, 'generation_found'): s.generation_found = gen
            
            self.update_hall_of_fame(top_50_strats, gen, horizon=horizon)

            # 5. Selection for Next Gen
            num_elite = int(self.pop_size * config.ELITE_PERCENTAGE)
            elites = [self.population[i] for i in sorted_indices[:num_elite]]
            
            new_population = elites[:50] # Keep top 50 strictly
            
            # Elite Mutation
            n_mutants = int(self.pop_size * 0.20)
            mutant_source = elites[:max(1, int(len(elites)*0.2))]
            for _ in range(n_mutants):
                if not mutant_source: break
                parent = random.choice(mutant_source)
                child = Strategy(name=f"Mutant_{random.randint(1000,9999)}")
                child.long_genes = [g.copy() for g in parent.long_genes]
                child.short_genes = [g.copy() for g in parent.short_genes]
                
                genes_to_mutate = child.long_genes + child.short_genes
                if genes_to_mutate:
                    target_genes = random.sample(genes_to_mutate, min(len(genes_to_mutate), 2))
                    for g in target_genes: g.mutate(self.factory.features)
                child.cleanup()
                child.recalculate_concordance()
                new_population.append(child)
            
            # Immigration
            n_immigrants = int(self.pop_size * config.IMMIGRATION_PERCENTAGE)
            for _ in range(n_immigrants):
                new_population.append(self.factory.create_strategy((config.GENE_COUNT_MIN, config.GENE_COUNT_MAX)))
            
            # Crossover
            while len(new_population) < self.pop_size:
                p1, p2 = random.sample(elites, 2)
                child = self.crossover(p1, p2)
                mut_rate = config.MUTATION_RATE
                for g in child.long_genes + child.short_genes:
                    if random.random() < mut_rate: g.mutate(self.factory.features)
                child.cleanup()
                child.recalculate_concordance()
                new_population.append(child)
                
            self.population = new_population
            print(f"  Gen {gen} completed in {time.time()-start_time:.2f}s | HOF Size: {len(self.hall_of_fame)}")

        # --- FINAL SELECTION ---
        # The Hall of Fame is already filtered for diversity and sorted by fitness.
        print(f"\n--- üõ°Ô∏è FINAL SELECTION (Using Diverse HOF) ---")
        
        top_candidates = [entry['strat'] for entry in self.hall_of_fame[:100]]
        val_fitness_map = {entry['strat'].name: entry['fit'] for entry in self.hall_of_fame[:100]}
        
        filtered_candidates = []
        filtered_data = []
        
        if top_candidates:
            # 1. Evaluate on Test Set (OOS)
            test_res = self.backtester.evaluate_population(top_candidates, set_type='test', return_series=False, prediction_mode=False, time_limit=horizon, min_trades=10)
            
            # Generate Signals for all sets
            val_start, val_end = self.backtester.train_idx, self.backtester.val_idx
            test_start, test_end = self.backtester.val_idx, len(self.backtester.open_vec)
            train_start, train_end = 0, self.backtester.train_idx
            
            full_signals = self.backtester.generate_signal_matrix(top_candidates)
            
            train_signals = full_signals[train_start:train_end]
            val_signals = full_signals[val_start:val_end]
            test_signals = full_signals[test_start:test_end]

            # Batch Sim Validation
            val_rets_batch, _ = self.backtester.run_simulation_batch(
                val_signals, top_candidates, self.backtester.open_vec[val_start:val_end],
                self.backtester.times_vec.iloc[val_start:val_end] if hasattr(self.backtester.times_vec, 'iloc') else self.backtester.times_vec[val_start:val_end],
                time_limit=horizon, highs=self.backtester.high_vec[val_start:val_end], lows=self.backtester.low_vec[val_start:val_end], atr=self.backtester.atr_vec[val_start:val_end]
            )
            
            # Batch Sim Test
            test_rets_batch, _ = self.backtester.run_simulation_batch(
                test_signals, top_candidates, self.backtester.open_vec[test_start:test_end],
                self.backtester.times_vec.iloc[test_start:test_end] if hasattr(self.backtester.times_vec, 'iloc') else self.backtester.times_vec[test_start:test_end],
                time_limit=horizon, highs=self.backtester.high_vec[test_start:test_end], lows=self.backtester.low_vec[test_start:test_end], atr=self.backtester.atr_vec[test_start:test_end]
            )

            # Batch Sim Train
            train_rets_batch, _ = self.backtester.run_simulation_batch(
                train_signals, top_candidates, self.backtester.open_vec[train_start:train_end],
                self.backtester.times_vec.iloc[train_start:train_end] if hasattr(self.backtester.times_vec, 'iloc') else self.backtester.times_vec[train_start:train_end],
                time_limit=horizon, highs=self.backtester.high_vec[train_start:train_end], lows=self.backtester.low_vec[train_start:train_end], atr=self.backtester.atr_vec[train_start:train_end]
            )

            best_rejected_name = None
            best_rejected_min_ret = -999.0
            best_rejected_details = ""

            # 2. Strict Filtering Loop
            for i, s in enumerate(top_candidates):
                if i >= len(test_res): break
                
                # Metric Extraction
                train_r_vec = train_rets_batch[:, i]
                val_r_vec = val_rets_batch[:, i]
                test_r_vec = test_rets_batch[:, i]
                
                train_ret = float(np.sum(train_r_vec))
                val_ret = float(np.sum(val_r_vec))
                test_ret = float(test_res.iloc[i]['total_return']) # Use dataframe derived metric
                
                # --- GATEKEEPING ---
                thresh = config.MIN_RETURN_THRESHOLD
                rejection_reason = []
                if test_ret < thresh: rejection_reason.append(f"Test({test_ret*100:.2f}%)")
                if train_ret < thresh: rejection_reason.append(f"Train({train_ret*100:.2f}%)")
                if val_ret < thresh: rejection_reason.append(f"Val({val_ret*100:.2f}%)")
                
                if rejection_reason:
                    # Track closest call
                    min_ret_here = min(train_ret, val_ret, test_ret)
                    if min_ret_here > best_rejected_min_ret:
                        best_rejected_min_ret = min_ret_here
                        best_rejected_name = s.name
                        best_rejected_details = f"Train: {train_ret*100:.2f}%, Val: {val_ret*100:.2f}%, Test: {test_ret*100:.2f}%"
                    continue
                
                # --- SENSITIVITY ANALYSIS (Jitter Test) ---
                # Ensure strategy isn't overfit to specific parameters (5% noise test)
                if val_ret > 0:
                    clones = []
                    for _ in range(3):
                        clone = Strategy(name=f"{s.name}_jit", long_genes=[g.copy() for g in s.long_genes], short_genes=[g.copy() for g in s.short_genes], min_concordance=s.min_concordance)
                        for g in clone.long_genes + clone.short_genes:
                            if hasattr(g, 'threshold'): g.threshold *= random.uniform(0.95, 1.05)
                            if hasattr(g, 'window') and isinstance(g.window, int): 
                                g.window = max(2, int(g.window * random.uniform(0.95, 1.05)))
                        clones.append(clone)
                    
                    try:
                        # Evaluate clones on Validation set
                        clone_stats = self.backtester.evaluate_population(clones, set_type='validation', return_series=False, time_limit=horizon)
                        if not clone_stats.empty:
                            min_clone_ret = clone_stats['total_return'].min()
                            # Requirement: Worst case scenario must retain 50% of performance (or strictly positive if margin is thin)
                            jitter_thresh = val_ret * 0.5
                            if min_clone_ret < jitter_thresh:
                                # print(f"    Rejected {s.name}: Failed Jitter Test (Val: {val_ret:.4f} -> Worst: {min_clone_ret:.4f})")
                                continue
                    except Exception as e:
                        print(f"Warning: Jitter test error for {s.name}: {e}")
                        continue

                # DSR/PSR Calcs (Only for survivors)
                val_sr = estimated_sharpe_ratio(val_r_vec, config.ANNUALIZATION_FACTOR)
                dsr_val = deflated_sharpe_ratio(
                    observed_sr=val_sr, returns=val_r_vec, n_trials=self.total_strategies_evaluated,
                    var_returns=np.var(val_r_vec), skew_returns=scipy.stats.skew(val_r_vec), kurt_returns=scipy.stats.kurtosis(val_r_vec),
                    annualization_factor=config.ANNUALIZATION_FACTOR
                )
                
                test_sr = estimated_sharpe_ratio(test_r_vec, config.ANNUALIZATION_FACTOR)
                psr_test = deflated_sharpe_ratio(
                    observed_sr=test_sr, returns=test_r_vec, n_trials=1,
                    var_returns=np.var(test_r_vec), skew_returns=scipy.stats.skew(test_r_vec), kurt_returns=scipy.stats.kurtosis(test_r_vec),
                    annualization_factor=config.ANNUALIZATION_FACTOR
                )
                
                # Build Data Dict
                strat_data = s.to_dict()
                strat_data['test_sortino'] = float(test_res.iloc[i]['sortino'])
                strat_data['test_return'] = test_ret
                strat_data['test_trades'] = int(test_res.iloc[i]['trades'])
                strat_data['train_return'] = train_ret
                strat_data['val_return'] = val_ret
                strat_data['robust_score'] = float(val_fitness_map.get(s.name, -999.0)) 
                strat_data['dsr_val'] = float(dsr_val)
                strat_data['psr_test'] = float(psr_test)
                strat_data['training_id'] = self.training_id
                strat_data['generation'] = getattr(s, 'generation_found', -1)
                
                filtered_candidates.append(s)
                filtered_data.append(strat_data)

            # --- PORTFOLIO & SAVING ---
            output = filtered_data
            
            if not output:
                print(f"‚ùå No strategies passed the strict Train/Val/Test filters (Threshold: {config.MIN_RETURN_THRESHOLD*100:.2f}%).")
                if best_rejected_name:
                    print(f"   üëÄ Closest Candidate: {best_rejected_name}")
                    print(f"      Stats: {best_rejected_details}")
                return

            print(f"‚úÖ {len(output)} Strategies passed final filtering.")
            
            # Save Apex Strategies (Only the good ones!)
            os.makedirs(config.DIRS['STRATEGIES_DIR'], exist_ok=True)
            out_path = os.path.join(config.DIRS['STRATEGIES_DIR'], f"apex_strategies_{horizon}.json")
            
            print("\n--- üß© CONSTRUCTING PORTFOLIO ---")
            
            # Portfolio Logic using filtered_candidates
            # Need signals for filtered candidates only
            # Re-slice from full_signals is tricky because indices shifted.
            # Simpler to just re-generate or map indices?
            # Mapping: original index i corresponds to s. We need to know which 'i' in full_signals corresponds to filtered list.
            
            # Actually, we can't easily map back if we filtered.
            # Let's just assume simple Top 5 selection from filtered list based on Robust Score
            
            # Sort by Robust Score
            filtered_data.sort(key=lambda x: x['robust_score'], reverse=True)
            
            # Simple Portfolio Selection (Top 5 Uncorrelated)
            portfolio = []
            # We need to re-gen signals for correlation check if we want to be precise
            # But for simplicity, let's just save the file. The Mutex Optimizer handles the real portfolio.
            
            # Save to Apex File
            # Merge with existing
            existing_data = []
            if os.path.exists(out_path):
                try:
                    with open(out_path, "r") as f: existing_data = json.load(f)
                except: pass
            
            combined = existing_data + output
            combined.sort(key=lambda x: x.get('robust_score', -999), reverse=True)
            
            # Dedup
            seen = set()
            unique = []
            for s in combined:
                if s['name'] not in seen:
                    unique.append(s)
                    seen.add(s['name'])
            
            with open(out_path, "w") as f: json.dump(unique[:1000], f, indent=4)
            print(f"üíæ Saved results to {out_path}")
            
            print("\n  Top Horizon Champions:")
            for i, s in enumerate(unique[:5]):
                print(f"    {i+1}. {s['name']} (Robust: {s['robust_score']:.2f} | Test: {s['test_return']*100:.2f}%)")

            # --- Persist to Global Inbox (Accumulate ALL finds) ---
            inbox_path = config.DIRS['STRATEGY_INBOX']
            inbox_data = []
            if os.path.exists(inbox_path):
                try:
                    with open(inbox_path, "r") as f: inbox_data = json.load(f)
                except: pass
            
            new_inbox_count = 0
            for s_data in output:
                # Double check not needed, already filtered.
                
                # Deduplicate by name against existing inbox
                if not any(x['name'] == s_data['name'] for x in inbox_data):
                     s_data['horizon'] = horizon 
                     inbox_data.append(s_data)
                     new_inbox_count += 1
            
            if new_inbox_count > 0:
                with open(inbox_path, "w") as f: json.dump(inbox_data, f, indent=4)
                print(f"üì¶ Added {new_inbox_count} new strategies to Inbox: {inbox_path}")
            # -------------------------------
        else:
            print("No strategies survived.")

import argparse

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--survivors", type=str, required=True)
    parser.add_argument("--horizon", type=int, default=60)
    parser.add_argument("--pop_size", type=int, default=5000)
    parser.add_argument("--gens", type=int, default=50)
    args = parser.parse_args()

    if not os.path.exists(config.DIRS['FEATURE_MATRIX']):
        print("‚ùå Feature Matrix not found.")
        exit(1)
        
    bars_df = pd.read_parquet(config.DIRS['FEATURE_MATRIX'])
    
    factory = EvolutionaryAlphaFactory(
        bars_df, 
        args.survivors, 
        population_size=args.pop_size, 
        generations=args.gens,
        target_col='log_ret',
        prediction_mode=False
    )
    
    try:
        factory.evolve(horizon=args.horizon)
    finally:
        factory.cleanup()
    
    # Old cleanup block removed as it is now handled by factory.cleanup()
